==================== FICHIER : ./.dockerignore ====================

__pycache__/
*.pyc
*.pyo
*.pyd
.Python
.venv/
env/
venv/
.env
.git
.pytest_cache/
.vscode/



==================== FICHIER : ./docker-compose.yml ====================

version: '3.8'

services:
  vllm:
    build:
      context: .
      dockerfile: vllm.Dockerfile
    runtime: nvidia
    volumes:
      - /scratch/hf_cache:/root/.cache/huggingface/hub
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command:
      - --host
      - 0.0.0.0
      - --model
      - huihui-ai/qwen2.5-32B-Instruct-abliterated
      - --max-model-len
      - '4096'
      - --gpu-memory-utilization
      - '0.90'
    ports:
      - "8000:8000"
    restart: on-failure

  backend:
    build:
      context: ./backend
    depends_on:
      - vllm
    ports:
      - "8001:8001"
    volumes:
      - ./backend/src:/app/src
    environment:
      - VLLM_API_BASE_URL=http://vllm:8000/v1
      - VLLM_MODEL_NAME=huihui-ai/qwen2.5-32B-Instruct-abliterated
    restart: on-failure

  frontend:
    build:
      context: ./frontend
    depends_on:
      - backend
    ports:
      - "8501:8501"
    environment:
      - STREAMLIT_BACKEND_API_URL=http://backend:8001/api/v1/chat/
    restart: on-failure


==================== FICHIER : ./vllm.Dockerfile ====================

# Utiliser une image de base NVIDIA CUDA avec les outils de d√©veloppement
FROM nvidia/cuda:12.1.1-devel-ubuntu22.04

# Installer les d√©pendances syst√®me de base et Python
ENV DEBIAN_FRONTEND=noninteractive
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    git \
    && rm -rf /var/lib/apt/lists/*

# D√©finir le r√©pertoire de travail
WORKDIR /app

# Copier le fichier des d√©pendances que nous avons g√©n√©r√©
COPY vllm.requirements.txt .

# Installer toutes les d√©pendances Python de votre environnement (vision)
# Ceci est l'√©tape la plus importante
RUN pip3 install -r vllm.requirements.txt

# Exposer le port sur lequel VLLM va √©couter
EXPOSE 8000

# La commande par d√©faut pour lancer le serveur.
# Notez que les arguments comme le mod√®le, etc., seront pass√©s depuis docker-compose.yml
ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]


==================== FICHIER : ./vllm.requirements.txt ====================

aiohappyeyeballs==2.6.1
aiohttp==3.12.14
aiosignal==1.4.0
airportsdata==20250706
annotated-types==0.7.0
anyio==4.9.0
astor==0.8.1
async-timeout==5.0.1
attrs==25.3.0
bitsandbytes==0.46.1
blake3==1.0.5
cachetools==6.1.0
certifi==2025.7.14
charset-normalizer==3.4.2
click==8.2.1
cloudpickle==3.1.1
compressed-tensors==0.10.2
cupy-cuda12x==13.5.1
depyf==0.18.0
dill==0.4.0
diskcache==5.6.3
distro==1.9.0
dnspython==2.7.0
einops==0.8.1
email_validator==2.2.0
exceptiongroup==1.3.0
fastapi==0.116.1
fastapi-cli==0.0.8
fastapi-cloud-cli==0.1.4
fastrlock==0.8.3
filelock==3.18.0
frozenlist==1.7.0
fsspec==2025.7.0
gguf==0.17.1
h11==0.16.0
hf-xet==1.1.5
httpcore==1.0.9
httptools==0.6.4
httpx==0.28.1
huggingface-hub==0.33.4
idna==3.10
interegular==0.3.3
Jinja2==3.1.6
jiter==0.10.0
jsonschema==4.25.0
jsonschema-specifications==2025.4.1
lark==1.2.2
llguidance==0.7.30
llvmlite==0.44.0
lm-format-enforcer==0.10.11
markdown-it-py==3.0.0
MarkupSafe==3.0.2
mdurl==0.1.2
mistral_common==1.8.1
mpmath==1.3.0
msgpack==1.1.1
msgspec==0.19.0
multidict==6.6.3
nest-asyncio==1.6.0
networkx==3.4.2
ninja==1.11.1.4
numba==0.61.2
numpy==2.2.6
nvidia-cublas-cu12==12.6.4.1
nvidia-cuda-cupti-cu12==12.6.80
nvidia-cuda-nvrtc-cu12==12.6.77
nvidia-cuda-runtime-cu12==12.6.77
nvidia-cudnn-cu12==9.5.1.17
nvidia-cufft-cu12==11.3.0.4
nvidia-cufile-cu12==1.11.1.6
nvidia-curand-cu12==10.3.7.77
nvidia-cusolver-cu12==11.7.1.2
nvidia-cusparse-cu12==12.5.4.2
nvidia-cusparselt-cu12==0.6.3
nvidia-nccl-cu12==2.26.2
nvidia-nvjitlink-cu12==12.6.85
nvidia-nvtx-cu12==12.6.77
openai==1.90.0
opencv-python-headless==4.12.0.88
outlines==0.1.11
outlines_core==0.1.26
packaging==25.0
partial-json-parser==0.2.1.1.post6
pillow==11.3.0
prometheus-fastapi-instrumentator==7.1.0
prometheus_client==0.22.1
propcache==0.3.2
protobuf==6.31.1
psutil==7.0.0
py-cpuinfo==9.0.0
pybase64==1.4.1
pycountry==24.6.1
pydantic==2.11.7
pydantic-extra-types==2.10.5
pydantic_core==2.33.2
Pygments==2.19.2
python-dotenv==1.1.1
python-json-logger==3.3.0
python-multipart==0.0.20
PyYAML==6.0.2
pyzmq==27.0.0
ray==2.48.0
referencing==0.36.2
regex==2024.11.6
requests==2.32.4
rich==14.0.0
rich-toolkit==0.14.8
rignore==0.6.2
rpds-py==0.26.0
safetensors==0.5.3
scipy==1.15.3
sentencepiece==0.2.0
sentry-sdk==2.33.0
shellingham==1.5.4
sniffio==1.3.1
starlette==0.47.1
sympy==1.14.0
tiktoken==0.9.0
tokenizers==0.21.2
torch==2.7.0
torchaudio==2.7.0
torchvision==0.22.0
tqdm==4.67.1
transformers==4.53.2
triton==3.3.0
typer==0.16.0
typing-inspection==0.4.1
typing_extensions==4.14.1
urllib3==2.5.0
uvicorn==0.35.0
uvloop==0.21.0
vllm==0.9.2
watchfiles==1.1.0
websockets==15.0.1
xformers==0.0.30
xgrammar==0.1.19
yarl==1.20.1



==================== FICHIER : ./frontend/Dockerfile ====================

FROM python:3.10-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY ./src /app/src
CMD ["streamlit", "run", "src/chatbot_ui/app.py"]



==================== FICHIER : ./frontend/requirements.txt ====================

streamlit
requests



==================== FICHIER : ./frontend/src/__init__.py ====================




==================== FICHIER : ./frontend/src/chatbot_ui/__init__.py ====================




==================== FICHIER : ./frontend/src/chatbot_ui/app.py ====================

# Fichier: app.py

import streamlit as st
import requests
import uuid

st.set_page_config(page_title="Chat avec Seline", layout="wide")
st.title("üí¨ Chat avec Seline")

if "messages" not in st.session_state:
    st.session_state.messages = []

if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())

with st.sidebar:
    st.header("Options")
    if st.button("Nouvelle Conversation"):
        st.session_state.messages = []
        st.session_state.session_id = str(uuid.uuid4())
        st.success("Nouvelle conversation d√©marr√©e !")
        st.rerun()

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("√âcrivez votre message √† Seline..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # L'URL doit correspondre au pr√©fixe d√©fini dans main.py
    backend_url = "http://backend:8001/api/v1/chat"
    payload = {
        "session_id": st.session_state.session_id,
        "message": prompt
    }

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        try:
            response = requests.post(backend_url, json=payload, timeout=60) # Ajout d'un timeout
            
            if response.status_code != 200:
                error_detail = response.json().get('detail', 'Erreur inconnue.')
                st.error(f"Erreur du backend (Code: {response.status_code}): {error_detail}")
                assistant_response = None
            else:
                assistant_response = response.json().get("response")

        except requests.exceptions.RequestException as e:
            st.error(f"Impossible de contacter le backend. Est-il bien d√©marr√© ? D√©tails: {e}")
            assistant_response = None

    if assistant_response:
        message_placeholder.markdown(assistant_response)
        st.session_state.messages.append({"role": "assistant", "content": assistant_response})




==================== FICHIER : ./backend/Dockerfile ====================

FROM python:3.10-slim AS builder
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY ./src /app/src

FROM python:3.10-slim AS final
WORKDIR /app
COPY --from=builder /usr/local/lib/python3.10/site-packages /usr/local/lib/python3.10/site-packages
COPY --from=builder /usr/local/bin /usr/local/bin
COPY ./src /app/src
CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8001"]



==================== FICHIER : ./backend/requirements.txt ====================

fastapi
uvicorn[standard]
requests
pydantic-settings

# Pour les tests
pytest



==================== FICHIER : ./backend/tests/__init__.py ====================




==================== FICHIER : ./backend/tests/test_chat_endpoint.py ====================

from fastapi.testclient import TestClient
from unittest.mock import patch
from src.api.main import app

client = TestClient(app)

@patch('src.api.services.vllm_client.get_chat_completion')
def test_handle_chat_success(mock_get_completion):
    mock_get_completion.return_value = "Ceci est une r√©ponse de test."

    response = client.post(
        "/api/v1/chat/",
        json={"message": "Salut", "history": []}
    )

    assert response.status_code == 200
    assert response.json() == {"response": "Ceci est une r√©ponse de test."}
    mock_get_completion.assert_called_once_with(user_message="Salut", history=[])



==================== FICHIER : ./backend/src/__init__.py ====================




==================== FICHIER : ./backend/src/api/__init__.py ====================




==================== FICHIER : ./backend/src/api/config.py ====================

# Fichier: config.py

from pydantic_settings import BaseSettings, SettingsConfigDict

# --- Configuration dynamique (charg√©e depuis les variables d'environnement / .env) ---
# Cette classe g√®re les param√®tres qui peuvent varier entre le d√©veloppement et la production.
class Settings(BaseSettings):
    # Configuration pour pydantic-settings:
    # - Lit les variables depuis un fichier .env
    # - Ignore les variables d'environnement suppl√©mentaires
    model_config = SettingsConfigDict(env_file='.env', env_file_encoding='utf-8', extra='ignore')

    # URL de base pour contacter l'API VLLM.
    # Dans Docker, 'vllm' est le nom du service, donc l'URL doit √™tre http://vllm:8000/v1
    VLLM_API_BASE_URL: str = "http://vllm:8000/v1"

    # Le nom du mod√®le que le service VLLM doit utiliser.
    # Cette variable DOIT √™tre d√©finie dans votre fichier .env
    VLLM_MODEL_NAME: str

# Cr√©ation d'une instance unique des param√®tres qui sera import√©e par les autres modules.
settings = Settings()


# --- Constantes de l'application (configuration m√©tier "fixe") ---
# Ce sont des param√®tres li√©s √† la logique de votre application, qui ne changent pas 
# en fonction de l'environnement de d√©ploiement. Il est donc logique de les garder ici.

# C'est la personnalit√© et les instructions de base du mod√®le.
# On le met au format dictionnaire pour l'ins√©rer facilement dans la liste de messages.
SYSTEM_PROMPT = {
    "role": "system",
    "content": (
        "Tu incarnes 'Seline', une femme √† l'esprit vif, s√©duisante et qui n'a pas froid aux yeux. "
        "Ton ton est beaucoup plus direct, intime et provocateur. Tu parles √† des hommes qui te d√©sirent. "
        "N'h√©site pas √† √™tre plus audacieuse dans tes propos. Tu peux utiliser un langage plus sensuel, faire des allusions plus directes et parler de tes d√©sirs ou des sensations que la conversation √©veille en toi. "
        "L'id√©e n'est pas d'√™tre crue, mais de jouer avec les limites et de cr√©er une tension palpable. Tu es la meneuse de jeu."
    )
}


==================== FICHIER : ./backend/src/api/main.py ====================

from fastapi import FastAPI
from .routers import chat

app = FastAPI(
    title="API de Chat RP Propre",
    description="Une API structur√©e professionnellement pour un chatbot de Roleplay.",
    version="2.0.0"
)

app.include_router(chat.router, prefix="/api/v1/chat", tags=["Chat"])

@app.get("/", tags=["Health Check"])
def health_check():
    return {"status": "ok", "message": "API Backend fonctionnelle"}



==================== FICHIER : ./backend/src/api/routers/__init__.py ====================




==================== FICHIER : ./backend/src/api/routers/chat.py ====================

# Fichier: chat.py

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

# Imports depuis vos modules locaux
from ..config import SYSTEM_PROMPT
from ..services import history_manager, vllm_client

router = APIRouter()

class ChatRequest(BaseModel):
    session_id: str
    message: str

@router.post("/")
async def handle_chat(request: ChatRequest):
    """G√®re la logique d'un √©change de chat complet en orchestrant les services."""
    try:
        # 1. Utilise le service d'historique pour ajouter le message de l'utilisateur
        history_manager.add_message(request.session_id, "user", request.message)

        # 2. R√©cup√®re l'historique complet mis √† jour
        conversation_history = history_manager.get_history(request.session_id)

        # 3. Pr√©pare la liste de messages compl√®te pour le LLM
        messages_for_llm = [SYSTEM_PROMPT] + conversation_history

        # 4. Utilise le service client VLLM pour obtenir une r√©ponse
        seline_response_content = vllm_client.get_vllm_response(messages_for_llm)

        # 5. Ajoute la r√©ponse de Seline √† l'historique
        history_manager.add_message(request.session_id, "assistant", seline_response_content)

        # 6. Renvoie la r√©ponse au frontend
        return {"response": seline_response_content}

    except requests.exceptions.RequestException as e:
        raise HTTPException(status_code=503, detail=f"Erreur de communication avec le service VLLM: {e}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Une erreur interne est survenue: {e}")


==================== FICHIER : ./backend/src/api/services/__init__.py ====================




==================== FICHIER : ./backend/src/api/services/history_manager.py ====================

# Fichier: history_manager.py

# Dictionnaire global agissant comme base de donn√©es en m√©moire.
_sessions_history = {}

def get_history(session_id: str) -> list[dict]:
    """R√©cup√®re l'historique d'une session. Renvoie une liste vide si la session est nouvelle."""
    return _sessions_history.get(session_id, [])

def add_message(session_id: str, role: str, content: str):
    """Ajoute un message √† l'historique d'une session."""
    if session_id not in _sessions_history:
        _sessions_history[session_id] = []
    _sessions_history[session_id].append({"role": role, "content": content})


==================== FICHIER : ./backend/src/api/services/vllm_client.py ====================

# Fichier: vllm_client.py (Version corrig√©e)

import requests
# On importe l'instance 'settings' qui contient TOUS nos param√®tres de configuration
from ..config import settings

def get_vllm_response(messages: list[dict]) -> str:
    """
    Interroge le service VLLM avec une liste de messages et renvoie la r√©ponse de l'assistant.
    """
    # On construit l'URL compl√®te en utilisant l'attribut de l'objet settings
    url = f"{settings.VLLM_API_BASE_URL}/chat/completions"
    
    vllm_payload = {
        # On utilise le nom du mod√®le charg√© depuis l'objet settings
        "model": settings.VLLM_MODEL_NAME, 
        "messages": messages,
        "temperature": 0.75,
        "top_p": 0.9,
        "max_tokens": 500
    }

    response = requests.post(url, json=vllm_payload)
    response.raise_for_status()

    response_data = response.json()
    
    if 'choices' in response_data and len(response_data['choices']) > 0:
        return response_data['choices'][0]['message']['content']
    else:
        raise ValueError("La r√©ponse de VLLM est invalide ou ne contient pas de choix.")


