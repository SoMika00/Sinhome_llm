version: '3.8'

services:
  vllm:
    build:
      context: .
      dockerfile: vllm.Dockerfile
    runtime: nvidia
    volumes:
      - /scratch/hf_cache:/root/.cache/huggingface/hub
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command:
      - --host
      - 0.0.0.0
      - --model
      - huihui-ai/qwen2.5-32B-Instruct-abliterated
      - --max-model-len
      - '4096'
      - --gpu-memory-utilization
      - '0.90'
    ports:
      - "8000:8000"
    restart: on-failure

  backend:
    build:
      context: ./backend
    depends_on:
      - vllm
    ports:
      - "8001:8001"
    volumes:
      - ./backend/src:/app/src
    environment:
      - VLLM_API_BASE_URL=http://vllm:8000/v1
      - VLLM_MODEL_NAME=huihui-ai/qwen2.5-32B-Instruct-abliterated
    restart: on-failure

  frontend:
    build:
      context: ./frontend
    depends_on:
      - backend
    ports:
      - "8501:8501"
    environment:
      - STREAMLIT_BACKEND_API_URL=http://backend:8001/api/v1/chat/
    restart: on-failure