# ThinkingThoughtsÂ ğŸ§ ğŸ’¬

![Docker](https://img.shields.io/badge/Docker-Ready-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-Async%20100%25-green)
![Streamlit](https://img.shields.io/badge/Streamlit-UI-red)

> **BoÃ®te Ã  outils prÃªte Ã  lâ€™emploi pour concevoir, tester et dÃ©ployer des applications LLM basÃ©es sur vLLM, FastAPI et Streamlit.**
>
> * **2Â commandes** pour lancer lâ€™ensemble (DockerÂ Compose)
> * **Streaming tempsâ€‘rÃ©el** des tokens
> * **Stateless** grÃ¢ce Ã  RedisÂ : scale horizontal immÃ©diat
> * API **OpenAIâ€‘compatible**Â : swapâ€inÂ / swapâ€out dâ€™un vrai compte OpenAI sans changer une ligne de code

---

## Table des matiÃ¨res

* [1. PrÃ©sentation](#1-prÃ©sentation)
* [2. DÃ©marrage rapide](#2-dÃ©marrage-rapide)
* [3. Architecture](#3-architecture)
* [4. Configuration dÃ©taillÃ©e](#4-configuration-dÃ©taillÃ©e)
* [5. Utilisation de lâ€™API](#5-utilisation-de-lapi)
* [6. DÃ©ploiement en production](#6-dÃ©ploiement-en-production)
* [7. FAQ & DÃ©pannage](#7-faq--dÃ©pannage)
* [8. Annexes](#8-annexes)

---

## 1. PrÃ©sentation

ThinkingThoughts propose un **starterâ€‘kit microâ€‘services** destinÃ© aux dataâ€‘scientists et dÃ©veloppeurs dÃ©sirant :

1. ItÃ©rer rapidement sur des prototypes LLM (prompt engineering, RAG, etc.).
2. Tester localement un modÃ¨le **openâ€‘source** via [vLLM](https://github.com/vllm-project/vllm) avant de basculer si besoin vers lâ€™API OpenAI.
3. DÃ©ployer en un clic sur un cloud ou un cluster onâ€‘prem.

<details>
<summary>ğŸŒŸ FonctionnalitÃ©s majeures</summary>

| Domaine         | DÃ©tails                                                                   |
| --------------- | ------------------------------------------------------------------------- |
| **Backend**     | FastAPI 100Â % asynchrone (`httpx.AsyncClient`), CORS, OpenTelemetry hooks |
| **LLM Service** | vLLMÂ 0.4 avec support du streaming & batching                             |
| **Frontend**    | Streamlit (UI chat, selection de modÃ¨le, temperature slider, historique)  |
| **Persistance** | Sessions stockÃ©es dans Redis pour garantir la tolÃ©rance aux pannes        |
| **CIÂ /Â CD**     | Exemple de pipeline GitHubÂ Actions (lint â†’ test â†’ buildÂ image)            |

</details>

---

## 2. DÃ©marrage rapide

> **PrÃ©â€‘requisÂ :** DockerÂ â‰¥Â 25 et DockerÂ ComposeÂ v2.

```bash
# Clone
$ git clone https://github.com/<you>/thinking-thoughts.git
$ cd thinking-thoughts

# Configuration (copier puis adapter .env)
$ cp .env.sample .env

# Lancement
$ docker compose up --build -d

# Interface Web
$ open http://localhost:8501
```

ğŸ›  **AstuceÂ :** `docker compose logs -f backend` pour suivre les requÃªtes.

---

## 3. Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   WebSockets    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   HTTP/JSON    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Streamlit  â”‚ â€” â€” â€” â€” â€” â€” â€”â–¶ â”‚  FastAPI   â”‚ â€” â€” â€” â€” â€” â€” â€”â–¶ â”‚     vLLM      â”‚
â”‚   UI       â”‚                â”‚  Backend   â”‚               â”‚  Service      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–²                            â”‚                          â–²
        â”‚                            â–¼                          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Redis (chat history) â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

* **Streamlit**Â : chat en tempsâ€‘rÃ©el (streaming), gestion de lâ€™APIÂ Key, choix du modÃ¨le.
* **FastAPI**Â : authentification, throttling, app logic.
* **vLLM**Â : serveur GPU optimisÃ© (paginatedÂ KVâ€‘cache, speculative decodingâ€¦).

---

## 4. Configuration dÃ©taillÃ©e

Toutes les variables sont centralisÃ©es dans `.env` et chargÃ©es via **pydanticâ€‘settings**.

| Variable                  | Description                | Exemple                                |
| ------------------------- | -------------------------- | -------------------------------------- |
| `VLLM_API_BASE_URL`       | URL du service vLLM        | `http://vllm:8000`                     |
| `VLLM_MODEL_NAME`         | ModÃ¨le Ã  charger           | `mistralai/Mixtral-8x7B-Instruct-v0.1` |
| `REDIS_URL`               | DSN Redis                  | `redis://redis:6379/0`                 |
| `STREAMLIT_AUTH_REQUIRED` | Bloquer la UI sans APIÂ Key | `true`                                 |
| `MAX_TOKENS`              | Limite par rÃ©ponse         | `512`                                  |

### Changer de modÃ¨le

```bash
# .env
VLLM_MODEL_NAME=microsoft/Phi-3-mini-4k-instruct
```

RedÃ©marrez simplement le service `vllm` (`docker compose restart vllm`).

### DÃ©ployer sur GPU âš¡ï¸

```yaml
services:
  vllm:
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
```

---

## 5. Utilisation de lâ€™API

### Endpoint principal

```http
POST /api/chat HTTP/1.1
Content-Type: application/json
x-api-key: <YOUR_KEY>

{
  "session_id": "d5bcâ€¦",
  "messages": [
    {"role": "user", "content": "Explique FastAPI en 3 points"}
  ],
  "stream": false
}
```

RÃ©ponseÂ :

```json
{
  "assistant": "1. FastAPI est un framework Pythonâ€¦"
}
```

### Streaming SSE

DÃ©finissez `stream=true` et itÃ©rez cÃ´tÃ© clientÂ :

```python
import sseclient, requests
resp = requests.post(url, json=payload, stream=True, headers=headers)
for token in sseclient.SSEClient(resp):
    print(token.data, end="", flush=True)
```

### Autres endpoints

| VerbeÂ /Â Route  | RÃ´le                        |
| -------------- | --------------------------- |
| `GET /health`  | LivenessÂ / readiness probes |
| `GET /metrics` | Prometheus export           |

---

## 6. DÃ©ploiement en production

* **DockerÂ Swarm**Â : stack.yml fourni (`docker stack deploy -c stack.yml thinking`)
* **Kubernetes**Â : manifest exemples dans `k8s/` (ingress + HPA autoscale).
* **Traefik**Â : TLSÂ certificates + auth middleware prÃªt Ã  lâ€™emploi.
* **ObservabilitÃ©**Â : intÃ©gration Grafana + Loki (logs) et Prometheus (metrics).

### Benchmark

```bash
$ wrk -t4 -c32 -d30s -s scripts/wrk_chat.lua http://localhost:8000/api/chat
```

---

## 7. FAQ & DÃ©pannage

<details>
<summary>Le service vLLM consomme trop de mÃ©moireÂ ?</summary>

* RÃ©duisez `VLLM_MAX_NUM_SEQS` ou passez Ã  un modÃ¨le plus petit.
* Activez lâ€™option `swap_memory=True` si votre GPU le permet.

</details>

<details>
<summary>Je reÃ§ois 502Â BadÂ Gateway lors du streamingÂ ?</summary>

Assurezâ€‘vous que votre reverseâ€‘proxy (Traefik, Nginx) laisse passer les connexions HTTPÂ SSE sans timeout (<code>proxy\_read\_timeout 3600s</code>).

</details>

---

## 8. Annexes

* **Tests unitaires**Â : `pytest -q` (couverture >Â 85Â %).
* **Linting**Â : `ruff check . && black --check .`.
* **Scripts utilitaires**Â : export conversation â†’ Markdown, purge Redis, etc.

---

<sub>MITÂ Â©Â 2025Â michailÂ alberjaoui â€” Â«â€¯Maintenir lâ€™Ã©quilibre entre lâ€™ordre et le chaosâ€¯Â»Â ğŸ§˜â€â™‚ï¸</sub>
